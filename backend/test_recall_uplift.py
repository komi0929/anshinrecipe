#!/usr/bin/env python3
"""
Recall Uplift v1 - Before/After Testing
Compare legacy CSE vs stepwise retrieval performance
"""

import requests
import json
import time
from typing import List, Dict

# Test configuration
BACKEND_URL = "https://recipe-shield.preview.emergentagent.com"
API_BASE = f"{BACKEND_URL}/api"

# Golden Set queries for comparison
GOLDEN_SET = [
    {
        "query": "Âçµ ‰π≥ Â∞èÈ∫¶ „Å™„Åó „Éë„É≥„Ç±„Éº„Ç≠ „É¨„Ç∑„Éî",
        "allergens": "Âçµ,‰π≥,Â∞èÈ∫¶",
        "description": "Time-saving pancake recipe"
    },
    {
        "query": "‰π≥ Â∞èÈ∫¶ „Å™„Åó „Çπ„Éº„Éó È´ò„Åü„Çì„Å±„Åè „É¨„Ç∑„Éî", 
        "allergens": "‰π≥,Â∞èÈ∫¶",
        "description": "Health-focused soup recipe"
    },
    {
        "query": "Âçµ ‰π≥ „Å™„Åó „ÇØ„ÉÉ„Ç≠„Éº Á∞°Âçò „É¨„Ç∑„Éî",
        "allergens": "Âçµ,‰π≥",
        "description": "Beginner cookie recipe"
    },
    {
        "query": "Âçµ ‰π≥ Â∞èÈ∫¶ „Å™„Åó „Ç±„Éº„Ç≠ „Éë„Éº„ÉÜ„Ç£„Éº „É¨„Ç∑„Éî",
        "allergens": "Âçµ,‰π≥,Â∞èÈ∫¶",
        "description": "Event cake recipe"
    },
    {
        "query": "„Åù„Å∞ „Å™„Åó „Å§„ÇÜ „É¨„Ç∑„Éî",
        "allergens": "„Åù„Å∞",
        "description": "Buckwheat-free broth"
    },
    {
        "query": "„Éî„Éº„Éä„ÉÉ„ÉÑ „Å™„Åó Âíå„ÅàÁâ© „É¨„Ç∑„Éî",
        "allergens": "ËêΩËä±Áîü",
        "description": "Peanut-free salad"
    }
]

def test_search_endpoint(query: str, allergens: str, use_stepwise: bool, timeout: int = 30) -> Dict:
    """Test a single search endpoint call"""
    
    params = {
        "q": query,
        "allergens": allergens,
        "debug": "1",
        "use_stepwise": "1" if use_stepwise else "0"
    }
    
    start_time = time.time()
    
    try:
        response = requests.get(f"{API_BASE}/v1/search", params=params, timeout=timeout)
        response_time_ms = int((time.time() - start_time) * 1000)
        
        if response.status_code == 200:
            data = response.json()
            
            results = data.get("results", [])
            debug_info = data.get("debug", {})
            
            # Extract metrics
            metrics = {
                "success": True,
                "response_time_ms": response_time_ms,
                "results_count": len(results),
                "zero_results_after_safety": 1 if len(results) == 0 else 0
            }
            
            # Extract exclusion stats
            exclusion_stats = debug_info.get("exclusionStats", {})
            if exclusion_stats:
                metrics.update({
                    "retrieval_total": exclusion_stats.get("total_processed", 0),
                    "recipe_type_pass": exclusion_stats.get("recipe_type_pass", 0),
                    "safety_ok_count": exclusion_stats.get("safety_ok_count", 0),
                    "safety_ng_count": exclusion_stats.get("safety_allergen", 0),
                    "safety_ambiguous_count": exclusion_stats.get("safety_ambiguous", 0),
                    "non_recipe_filtered": exclusion_stats.get("non_recipe_filtered", 0)
                })
            
            # Extract stepwise metrics
            stepwise_metrics = debug_info.get("stepwiseMetrics", {})
            if stepwise_metrics:
                retrieval_passes = stepwise_metrics.get("retrieval_passes", [])
                metrics["stepwise_passes_count"] = len(retrieval_passes)
                
                performance = stepwise_metrics.get("performance", {})
                metrics.update({
                    "retrieval_ms": performance.get("retrieval_ms", 0),
                    "safety_ms": performance.get("safety_ms", 0),
                    "scoring_ms": performance.get("scoring_ms", 0)
                })
            
            # Check AnshinScore normalization
            if results:
                scores = [r.get("anshinScore", 0) for r in results]
                metrics["min_anshin_score"] = min(scores)
                metrics["max_anshin_score"] = max(scores)
                metrics["avg_anshin_score"] = sum(scores) / len(scores)
            
            return metrics
            
        else:
            return {
                "success": False,
                "response_time_ms": response_time_ms,
                "error": f"HTTP {response.status_code}",
                "results_count": 0,
                "zero_results_after_safety": 1
            }
            
    except Exception as e:
        return {
            "success": False,
            "response_time_ms": int((time.time() - start_time) * 1000),
            "error": str(e),
            "results_count": 0,
            "zero_results_after_safety": 1
        }

def run_before_after_comparison():
    """Run comprehensive before/after comparison"""
    
    print("=" * 80)
    print("üöÄ RECALL UPLIFT v1 - COMPREHENSIVE BEFORE/AFTER TESTING")
    print("=" * 80)
    
    results = []
    
    for i, test_case in enumerate(GOLDEN_SET, 1):
        print(f"\nüìã Test Case {i}/6: {test_case['description']}")
        print(f"Query: {test_case['query']}")
        print(f"Allergens: {test_case['allergens']}")
        print("-" * 60)
        
        # BEFORE: Legacy CSE
        print("üîç Testing BEFORE (Legacy CSE)...")
        before_metrics = test_search_endpoint(
            test_case["query"], 
            test_case["allergens"], 
            use_stepwise=False
        )
        
        time.sleep(2)  # Brief pause between tests
        
        # AFTER: Stepwise Retrieval
        print("üîç Testing AFTER (Stepwise v1)...")
        after_metrics = test_search_endpoint(
            test_case["query"], 
            test_case["allergens"], 
            use_stepwise=True
        )
        
        # Compare results
        comparison = {
            "query": test_case["query"],
            "description": test_case["description"],
            "allergens": test_case["allergens"],
            "before": before_metrics,
            "after": after_metrics
        }
        
        results.append(comparison)
        
        # Print comparison
        print(f"\nüìä RESULTS COMPARISON:")
        print(f"   Results Count:     BEFORE {before_metrics['results_count']} ‚Üí AFTER {after_metrics['results_count']}")
        print(f"   Response Time:     BEFORE {before_metrics['response_time_ms']}ms ‚Üí AFTER {after_metrics['response_time_ms']}ms")
        
        if before_metrics.get('safety_ok_count') is not None and after_metrics.get('safety_ok_count') is not None:
            print(f"   Safety OK Count:   BEFORE {before_metrics['safety_ok_count']} ‚Üí AFTER {after_metrics['safety_ok_count']}")
        
        if after_metrics.get('stepwise_passes_count'):
            print(f"   Stepwise Passes:   {after_metrics['stepwise_passes_count']}")
        
        # Show improvement
        result_improvement = after_metrics['results_count'] - before_metrics['results_count']
        if result_improvement > 0:
            print(f"   üéâ IMPROVEMENT: +{result_improvement} results")
        elif result_improvement == 0:
            print(f"   ‚û°Ô∏è  STABLE: No change")
        else:
            print(f"   ‚ö†Ô∏è  REGRESSION: {result_improvement} results")
        
        time.sleep(1)  # Brief pause between test cases
    
    # Overall summary
    print("\n" + "=" * 80)
    print("üìä OVERALL BEFORE/AFTER SUMMARY")
    print("=" * 80)
    
    total_before_results = sum(r["before"]["results_count"] for r in results)
    total_after_results = sum(r["after"]["results_count"] for r in results)
    
    before_zero_count = sum(r["before"]["zero_results_after_safety"] for r in results)
    after_zero_count = sum(r["after"]["zero_results_after_safety"] for r in results)
    
    avg_before_time = sum(r["before"]["response_time_ms"] for r in results) / len(results)
    avg_after_time = sum(r["after"]["response_time_ms"] for r in results) / len(results)
    
    print(f"üìà Total Results:           BEFORE {total_before_results} ‚Üí AFTER {total_after_results}")
    print(f"üìâ Zero Results Cases:      BEFORE {before_zero_count} ‚Üí AFTER {after_zero_count}")
    print(f"‚è±Ô∏è  Average Response Time:   BEFORE {avg_before_time:.0f}ms ‚Üí AFTER {avg_after_time:.0f}ms")
    
    overall_improvement = total_after_results - total_before_results
    zero_improvement = before_zero_count - after_zero_count
    
    print(f"\nüéØ KEY IMPROVEMENTS:")
    print(f"   ‚Ä¢ Total Result Increase: +{overall_improvement} recipes")
    print(f"   ‚Ä¢ Zero Result Reduction: -{zero_improvement} cases")
    
    if overall_improvement > 0:
        print(f"   ‚Ä¢ Recall Improvement: +{(overall_improvement/max(1,total_before_results)*100):.1f}%")
    
    # AnshinScore normalization check
    all_scores = []
    for result in results:
        if result["after"].get("max_anshin_score"):
            all_scores.append(result["after"]["max_anshin_score"])
    
    if all_scores:
        print(f"\nüé® SCORE NORMALIZATION:")
        print(f"   ‚Ä¢ Max AnshinScore: {max(all_scores)} (should be ‚â§100)")
        print(f"   ‚Ä¢ Min AnshinScore: {min(all_scores)} (should be ‚â•0)")
        
        if max(all_scores) <= 100 and min(all_scores) >= 0:
            print(f"   ‚úÖ Score normalization working correctly")
        else:
            print(f"   ‚ùå Score normalization needs fixing")
    
    return results

if __name__ == "__main__":
    results = run_before_after_comparison()
    print(f"\n‚úÖ Testing completed. {len(results)} test cases processed.")